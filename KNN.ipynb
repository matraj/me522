{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYDE 522 Project Code\n",
    "Chang Li, Maathusan Rajendram, Anastasia Santasheva, Evan Yeung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard useful packages \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# validation & normalization methods\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, LeaveOneOut\n",
    "\n",
    "# accuracy, MSE, log loss & timer methods\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import log_loss\n",
    "from time import time\n",
    "\n",
    "# dim reduction & classification methods \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "\n",
    "# make matplotlib to show plots inline\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configuration\n",
    "* Select options for method validation\n",
    "* Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. set dataset\n",
    "ENABLE_POR_DATA = False     # set Portugese course dataset\n",
    "ENABLE_MAT_DATA = True    # set Math course dataset\n",
    "\n",
    "# 2. set supervised approach for G3\n",
    "ENABLE_BINARY_TARGET = False       # sets G3 to binary\n",
    "ENABLE_5LEVEL_TARGET = True        # set G3 to five-level scale\n",
    "ENABLE_REGRESSION_TARGET = False   # set G3 to current state for regression\n",
    "\n",
    "# 3. set dimensionaltiy reduction method - set both to false for none\n",
    "ENABLE_PCN = False\n",
    "ENABLE_LDA = True\n",
    "\n",
    "# 4. set validation type\n",
    "ENABLE_KFOLD = True\n",
    "ENABLE_LOO = False\n",
    "\n",
    "# 5. set final test\n",
    "ENABLE_TEST = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load  Dataset\n",
    "* Select a data set (Portugese course or Math course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((316, 33), (79, 33))"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data from csv\n",
    "if (ENABLE_POR_DATA):\n",
    "    dataframe = pd.read_csv('student-por-train.csv', usecols = range(0,33)) \n",
    "    dataframe_test = pd.read_csv('student-por-test.csv', usecols = range(0,33)) \n",
    "elif (ENABLE_MAT_DATA): \n",
    "    dataframe = pd.read_csv('student-mat-train.csv', usecols = range(0,33))\n",
    "    dataframe_test = pd.read_csv('student-mat-test.csv', usecols = range(0,33))\n",
    "\n",
    "dataset = dataframe.values\n",
    "dataset_test = dataframe_test.values\n",
    "dataset.shape, dataset_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "* Convert nominal attributes with Integer + One-Hot Encoding\n",
    "* Selects supervised approache for G3\n",
    "* NOTE: if we want we can also split further into A,B,C (A= all cols, B=same as A without G2, C=same as B without G1)\n",
    "    * But leaving this out for now since we know A gives best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# helper functions for preprocessing\n",
    "def convertToBinary(df, num_cols):\n",
    "    df.loc[(df.G3 < 10), 'G3'] = 0\n",
    "    df.loc[(df.G3 >= 10), 'G3'] = 1\n",
    "    \n",
    "    G3 = df.values[:,num_cols-1]\n",
    "    return G3\n",
    "\n",
    "def convertToFiveLevel(df, num_cols):\n",
    "    df.loc[(df.G3 <= 9), 'G3'] = 0\n",
    "    df.loc[(df.G3 > 9) & (df.G3 <= 11), 'G3'] = 1\n",
    "    df.loc[(df.G3 > 11) & (df.G3 <= 13), 'G3'] = 2\n",
    "    df.loc[(df.G3 > 13) & (df.G3 <= 16), 'G3'] = 3\n",
    "    df.loc[(df.G3 > 16), 'G3'] = 4\n",
    "    \n",
    "    G3 = df.values[:,num_cols-1]\n",
    "    return G3   \n",
    "\n",
    "def oneHotEncode(df, num_cols):\n",
    "    df = df.drop(labels='G3', axis=1)\n",
    "    cols_to_transform = [\n",
    "                        'school',\n",
    "                        'sex',\n",
    "                        'address',\n",
    "                        'famsize',\n",
    "                        'Pstatus',\n",
    "                        'Mjob',                        \n",
    "                        'Fjob',\n",
    "                        'reason',\n",
    "                        'guardian',\n",
    "                        'famsup',\n",
    "                        'schoolsup',\n",
    "                        'paid',\n",
    "                        'activities',\n",
    "                        'nursery',                        \n",
    "                        'higher',\n",
    "                        'internet',\n",
    "                        'romantic',\n",
    "                        ]\n",
    "    hot_encoded_df = pd.get_dummies(df, columns = cols_to_transform)\n",
    "    \n",
    "    attributes = hot_encoded_df.values\n",
    "    return attributes\n",
    "\n",
    "def normalizeData(train_data, val_data):\n",
    "    scaler = StandardScaler().fit(train_data)\n",
    "    train_data = scaler.transform(train_data)\n",
    "    val_data = scaler.transform(val_data)\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "# shuffle dataset\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "# find col length\n",
    "num_cols = dataset.shape[1]\n",
    "\n",
    "# labels of Y\n",
    "labels = []\n",
    "\n",
    "# split one-hot encoded attributes (X) and G3 (Y)\n",
    "X = oneHotEncode(dataframe, num_cols)\n",
    "X_tst = oneHotEncode(dataframe_test, num_cols)\n",
    "\n",
    "# selects supervised approach for G3\n",
    "if (ENABLE_BINARY_TARGET):\n",
    "    Y = convertToBinary(dataframe, num_cols).astype('int')\n",
    "    Y_tst = convertToBinary(dataframe_test, num_cols).astype('int')\n",
    "    labels = [0, 1]\n",
    "elif (ENABLE_5LEVEL_TARGET):\n",
    "    Y = convertToFiveLevel(dataframe, num_cols).astype('int')\n",
    "    Y_tst = convertToFiveLevel(dataframe_test, num_cols).astype('int')\n",
    "    labels = [0, 1, 2, 3, 4]\n",
    "elif (ENABLE_REGRESSION_TARGET):\n",
    "    Y = dataset[:,num_cols-1].astype('int')\n",
    "    Y_tst = dataset_test[:,num_cols-1].astype('int')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((316, 58), (316,), (79, 58), (79,))"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y\n",
    "X.shape, Y.shape, X_tst.shape, Y_tst.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "* PCA & LDA reduction methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcaReduction(train_data, val_data, n_comp):\n",
    "    pca = PCA(n_components=n_comp)\n",
    "    train_data = pca.fit_transform(train_data)\n",
    "    val_data = pca.transform(val_data)\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "def ldaReduction(train_data, train_target, val_data, n_comp):\n",
    "    lda = LinearDiscriminantAnalysis(n_components=n_comp)\n",
    "    train_data = lda.fit_transform(train_data, train_target)\n",
    "    val_data = lda.transform(val_data)\n",
    "    \n",
    "    return train_data, val_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Methods\n",
    "* k-Fold cross validation & Leave-one-out validation\n",
    "* data gets normalized here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to calculate accuracy (PCC) & RMSE\n",
    "def calcMetric(actual, predicted):\n",
    "    if(ENABLE_REGRESSION_TARGET): return (mean_squared_error(actual, predicted))**(0.5) # calculates RMSE\n",
    "    else: return accuracy_score(actual, predicted, normalize = True) # calculates PCC\n",
    "\n",
    "def kFoldValidation(n_comp, penalty, n_splits=10):\n",
    "    kFold = KFold(n_splits=n_splits)\n",
    "    \n",
    "    # run on validation data\n",
    "    val_results = []\n",
    "    train_results = []\n",
    "    log_loss_results = []\n",
    "    time_log = []\n",
    "    \n",
    "    for train_index, val_index in kFold.split(X):\n",
    "        #start timer, return avg time below\n",
    "        start = time()\n",
    "        \n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        Y_train, Y_val = Y[train_index], Y[val_index]\n",
    "        \n",
    "        # normalize data\n",
    "        X_train, X_val = normalizeData(X_train, X_val)\n",
    "        \n",
    "        # reduce dimensionality\n",
    "        if (ENABLE_PCN): X_train, X_val = pcaReduction(X_train, X_val, n_comp)\n",
    "        elif (ENABLE_LDA): X_train, X_val = ldaReduction(X_train, Y_train, X_val, n_comp)\n",
    "        \n",
    "        # build classifier for each set\n",
    "        clf = buildClf(X_train, Y_train, penalty)\n",
    "        \n",
    "        predicted = clf.predict(X_val)\n",
    "        val_accuracy = calcMetric(Y_val, predicted)\n",
    "        predicted = clf.predict(X_train)\n",
    "        train_accuracy = calcMetric(Y_train, predicted)\n",
    "        \n",
    "        # log loss calculation - for classification only\n",
    "        if(not ENABLE_REGRESSION_TARGET): log_loss_results.append(log_loss(Y_val, clf.predict_proba(X_val), labels=labels))\n",
    "        \n",
    "        val_results.append(val_accuracy)\n",
    "        train_results.append(train_accuracy)\n",
    "        time_log.append(time()-start)\n",
    "        \n",
    "    return np.mean(train_results, axis = 0), np.mean(val_results, axis = 0), np.mean(log_loss_results, axis = 0), np.mean(time_log, axis = 0)\n",
    "\n",
    "def looValidation(n_comp, penalty):\n",
    "    loo = LeaveOneOut()\n",
    "    \n",
    "    # run on validation data\n",
    "    val_results = []\n",
    "    train_results = []\n",
    "    Y_test_prob = []\n",
    "    log_loss_value = 0\n",
    "    time_log = []\n",
    "    \n",
    "    for train_index, val_index in loo.split(X):\n",
    "        #start timer, return avg time below\n",
    "        start = time()\n",
    "        \n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        Y_train, Y_val = Y[train_index], Y[val_index]\n",
    "        \n",
    "        # normalize data\n",
    "        X_train, X_val = normalizeData(X_train, X_val)\n",
    "        \n",
    "        # reduce dimensionality\n",
    "        if (ENABLE_PCN): X_train, X_val = pcaReduction(X_train, X_val, n_comp)\n",
    "        elif (ENABLE_LDA): X_train, X_val = ldaReduction(X_train, Y_train, X_val, n_comp)\n",
    "        \n",
    "        # build classifier for each set\n",
    "        clf = buildClf(X_train, Y_train, penalty)\n",
    "        \n",
    "        predicted = clf.predict(X_val)\n",
    "        val_accuracy = calcMetric(Y_val, predicted)\n",
    "        predicted = clf.predict(X_train)\n",
    "        train_accuracy = calcMetric(Y_train, predicted)\n",
    "        \n",
    "        # save probability for log loss calculation\n",
    "        if(not ENABLE_REGRESSION_TARGET): Y_test_prob.append(clf.predict_proba(X_val)[0])\n",
    "            \n",
    "        test_results.append(test_accuracy)\n",
    "        val_results.append(val_accuracy)\n",
    "        train_results.append(train_accuracy)\n",
    "        time_log.append(time()-start)\n",
    "    \n",
    "    # log loss calculation - for classification only\n",
    "    if(not ENABLE_REGRESSION_TARGET): log_loss_value = log_loss(Y, Y_test_prob, labels=labels)\n",
    "\n",
    "    return np.mean(train_results, axis = 0), np.mean(val_results, axis = 0), log_loss_value, np.mean(time_log, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Selection\n",
    "* Select classifer to validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVotingClassifier():\n",
    "    clf1 = SVC(C=10000, probability=True, kernel='rbf', gamma=0.0001)\n",
    "    clf2 = RandomForestClassifier(n_estimators = 100, criterion = \"entropy\")\n",
    "    clf3 = GaussianNB()\n",
    "\n",
    "    return VotingClassifier(estimators=[('svm', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')\n",
    "\n",
    "# method to build classifier\n",
    "def buildClf(train_data, train_target, penalty):\n",
    "    \n",
    "    if(ENABLE_REGRESSION_TARGET):\n",
    "#         model = SVR(C=0.0001, kernel='sigmoid', gamma=penalty) # for regression\n",
    "        model = KNeighborsClassifier(n_neighbors=penalty)\n",
    "#         model = SVR(C=penalty, kernel='linear') # for regression\n",
    "    else:\n",
    "#         model = SVC(C=penalty, probability=True, kernel='linear')\n",
    "#         model = getVotingClassifier()\n",
    "#         model = GaussianNB()\n",
    "#         model = MLPClassifier(solver = 'lbfgs')\n",
    "#         model = DecisionTreeClassifier(max_depth = None, max_features = penalty, criterion = \"entropy\")\n",
    "#         model = RandomForestClassifier(n_estimators = penalty, max_features=450, criterion = \"entropy\")\n",
    "        model = KNeighborsClassifier(n_neighbors=penalty)\n",
    "#         model = BaggingClassifier(DecisionTreeClassifier(max_features = penalty, criterion = \"entropy\"), max_samples=0.5, max_features=6)\n",
    "\n",
    "    model.fit(train_data, train_target)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Test Method\n",
    "* Run best classifier on unseen test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get accuracy/RMSE, logloss and time \n",
    "def runTestSet(X_train, Y_train, X_test, Y_test, n_comp, penalty):\n",
    "    \n",
    "    start = time()\n",
    "    test_log_loss = 0\n",
    "    \n",
    "    # normalize data\n",
    "    X_train, X_test = normalizeData(X_train, X_test)\n",
    "\n",
    "    # reduce dimensionality\n",
    "    if (ENABLE_PCN): X_train, X_test = pcaReduction(X_train, X_test, n_comp)\n",
    "    elif (ENABLE_LDA): X_train, X_test = ldaReduction(X_train, Y_train, X_test, n_comp)\n",
    "\n",
    "    # build classifier, predict and get accuracy\n",
    "    clf = buildClf(X_train, Y_train, penalty)\n",
    "    predicted = clf.predict(X_test)\n",
    "    test_accuracy = calcMetric(Y_test, predicted)\n",
    "    \n",
    "    # log loss calculation - for classification only\n",
    "    if(not ENABLE_REGRESSION_TARGET): test_log_loss = log_loss(Y_test, clf.predict_proba(X_test))\n",
    "        \n",
    "    # execution time calculation\n",
    "    test_time = time()-start\n",
    "    \n",
    "    return test_accuracy, test_log_loss, test_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain Accuracy, Log Loss & Error\n",
    "* Adjust array of penalty parameters\n",
    "* Graphs error and log loss - for classification only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hparam</th>\n",
       "      <th>Time</th>\n",
       "      <th>Acc</th>\n",
       "      <th>logloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.008345</td>\n",
       "      <td>0.657863</td>\n",
       "      <td>11.816997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.008035</td>\n",
       "      <td>0.702722</td>\n",
       "      <td>1.711525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0.010855</td>\n",
       "      <td>0.686391</td>\n",
       "      <td>0.912257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.010703</td>\n",
       "      <td>0.699093</td>\n",
       "      <td>0.813448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>0.674194</td>\n",
       "      <td>0.711887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0.020236</td>\n",
       "      <td>0.667742</td>\n",
       "      <td>0.630805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>0.026006</td>\n",
       "      <td>0.683770</td>\n",
       "      <td>0.665577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.024707</td>\n",
       "      <td>0.684173</td>\n",
       "      <td>0.736497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125</td>\n",
       "      <td>0.015860</td>\n",
       "      <td>0.649093</td>\n",
       "      <td>0.830131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150</td>\n",
       "      <td>0.016539</td>\n",
       "      <td>0.645867</td>\n",
       "      <td>0.943100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>175</td>\n",
       "      <td>0.018129</td>\n",
       "      <td>0.624093</td>\n",
       "      <td>1.057442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>0.019307</td>\n",
       "      <td>0.456855</td>\n",
       "      <td>1.167858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>225</td>\n",
       "      <td>0.019622</td>\n",
       "      <td>0.403024</td>\n",
       "      <td>1.274200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.007678</td>\n",
       "      <td>0.635988</td>\n",
       "      <td>12.572532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.009126</td>\n",
       "      <td>0.667641</td>\n",
       "      <td>2.483699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0.009139</td>\n",
       "      <td>0.693246</td>\n",
       "      <td>1.374452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.016717</td>\n",
       "      <td>0.664919</td>\n",
       "      <td>0.784737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>0.013170</td>\n",
       "      <td>0.655645</td>\n",
       "      <td>0.692703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0.010726</td>\n",
       "      <td>0.642944</td>\n",
       "      <td>0.700533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>0.012375</td>\n",
       "      <td>0.646069</td>\n",
       "      <td>0.727683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.014156</td>\n",
       "      <td>0.649194</td>\n",
       "      <td>0.802950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125</td>\n",
       "      <td>0.016174</td>\n",
       "      <td>0.642944</td>\n",
       "      <td>0.884559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150</td>\n",
       "      <td>0.016194</td>\n",
       "      <td>0.636694</td>\n",
       "      <td>0.982881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>175</td>\n",
       "      <td>0.017395</td>\n",
       "      <td>0.605343</td>\n",
       "      <td>1.082111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>0.018203</td>\n",
       "      <td>0.460181</td>\n",
       "      <td>1.181003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>225</td>\n",
       "      <td>0.019909</td>\n",
       "      <td>0.393548</td>\n",
       "      <td>1.280994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.007876</td>\n",
       "      <td>0.630242</td>\n",
       "      <td>12.770991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.008755</td>\n",
       "      <td>0.674093</td>\n",
       "      <td>1.977586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0.009044</td>\n",
       "      <td>0.664919</td>\n",
       "      <td>0.878980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.010118</td>\n",
       "      <td>0.661794</td>\n",
       "      <td>0.794676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>0.010726</td>\n",
       "      <td>0.633266</td>\n",
       "      <td>0.702819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0.011601</td>\n",
       "      <td>0.636391</td>\n",
       "      <td>0.709961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>0.012954</td>\n",
       "      <td>0.633367</td>\n",
       "      <td>0.760669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.014791</td>\n",
       "      <td>0.642944</td>\n",
       "      <td>0.827530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.630242</td>\n",
       "      <td>0.907073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150</td>\n",
       "      <td>0.016024</td>\n",
       "      <td>0.620867</td>\n",
       "      <td>0.995989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>175</td>\n",
       "      <td>0.017452</td>\n",
       "      <td>0.589516</td>\n",
       "      <td>1.090477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>0.018938</td>\n",
       "      <td>0.460081</td>\n",
       "      <td>1.185247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>225</td>\n",
       "      <td>0.020183</td>\n",
       "      <td>0.390323</td>\n",
       "      <td>1.283397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.007223</td>\n",
       "      <td>0.680141</td>\n",
       "      <td>11.047534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.008717</td>\n",
       "      <td>0.689919</td>\n",
       "      <td>1.175489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0.009860</td>\n",
       "      <td>0.674294</td>\n",
       "      <td>0.783010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.010260</td>\n",
       "      <td>0.648992</td>\n",
       "      <td>0.792060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>0.011318</td>\n",
       "      <td>0.652218</td>\n",
       "      <td>0.714929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0.011662</td>\n",
       "      <td>0.661492</td>\n",
       "      <td>0.729719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>0.012825</td>\n",
       "      <td>0.642944</td>\n",
       "      <td>0.782713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.014361</td>\n",
       "      <td>0.630343</td>\n",
       "      <td>0.849771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125</td>\n",
       "      <td>0.016037</td>\n",
       "      <td>0.614214</td>\n",
       "      <td>0.923948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150</td>\n",
       "      <td>0.016673</td>\n",
       "      <td>0.617641</td>\n",
       "      <td>1.012502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>175</td>\n",
       "      <td>0.017643</td>\n",
       "      <td>0.579839</td>\n",
       "      <td>1.102824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.463004</td>\n",
       "      <td>1.193188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>225</td>\n",
       "      <td>0.021926</td>\n",
       "      <td>0.393548</td>\n",
       "      <td>1.287232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  hparam      Time       Acc    logloss\n",
       "0      1  0.008345  0.657863  11.816997\n",
       "0     10  0.008035  0.702722   1.711525\n",
       "0     20  0.010855  0.686391   0.912257\n",
       "0     30  0.010703  0.699093   0.813448\n",
       "0     40  0.010283  0.674194   0.711887\n",
       "0     50  0.020236  0.667742   0.630805\n",
       "0     75  0.026006  0.683770   0.665577\n",
       "0    100  0.024707  0.684173   0.736497\n",
       "0    125  0.015860  0.649093   0.830131\n",
       "0    150  0.016539  0.645867   0.943100\n",
       "0    175  0.018129  0.624093   1.057442\n",
       "0    200  0.019307  0.456855   1.167858\n",
       "0    225  0.019622  0.403024   1.274200\n",
       "0      1  0.007678  0.635988  12.572532\n",
       "0     10  0.009126  0.667641   2.483699\n",
       "0     20  0.009139  0.693246   1.374452\n",
       "0     30  0.016717  0.664919   0.784737\n",
       "0     40  0.013170  0.655645   0.692703\n",
       "0     50  0.010726  0.642944   0.700533\n",
       "0     75  0.012375  0.646069   0.727683\n",
       "0    100  0.014156  0.649194   0.802950\n",
       "0    125  0.016174  0.642944   0.884559\n",
       "0    150  0.016194  0.636694   0.982881\n",
       "0    175  0.017395  0.605343   1.082111\n",
       "0    200  0.018203  0.460181   1.181003\n",
       "0    225  0.019909  0.393548   1.280994\n",
       "0      1  0.007876  0.630242  12.770991\n",
       "0     10  0.008755  0.674093   1.977586\n",
       "0     20  0.009044  0.664919   0.878980\n",
       "0     30  0.010118  0.661794   0.794676\n",
       "0     40  0.010726  0.633266   0.702819\n",
       "0     50  0.011601  0.636391   0.709961\n",
       "0     75  0.012954  0.633367   0.760669\n",
       "0    100  0.014791  0.642944   0.827530\n",
       "0    125  0.015713  0.630242   0.907073\n",
       "0    150  0.016024  0.620867   0.995989\n",
       "0    175  0.017452  0.589516   1.090477\n",
       "0    200  0.018938  0.460081   1.185247\n",
       "0    225  0.020183  0.390323   1.283397\n",
       "0      1  0.007223  0.680141  11.047534\n",
       "0     10  0.008717  0.689919   1.175489\n",
       "0     20  0.009860  0.674294   0.783010\n",
       "0     30  0.010260  0.648992   0.792060\n",
       "0     40  0.011318  0.652218   0.714929\n",
       "0     50  0.011662  0.661492   0.729719\n",
       "0     75  0.012825  0.642944   0.782713\n",
       "0    100  0.014361  0.630343   0.849771\n",
       "0    125  0.016037  0.614214   0.923948\n",
       "0    150  0.016673  0.617641   1.012502\n",
       "0    175  0.017643  0.579839   1.102824\n",
       "0    200  0.019000  0.463004   1.193188\n",
       "0    225  0.021926  0.393548   1.287232"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['hparam', 'Time', 'Acc', 'logloss']\n",
    "all_results = pd.DataFrame(columns=cols)\n",
    "\n",
    "# input reduced dimension - this can be ignored if none selected\n",
    "components_to_test = [1, 2, 3, 4]\n",
    "\n",
    "# input parameter iterations - can tune other params in classifer selection method above\n",
    "if(not ENABLE_TEST):\n",
    "    # hyperparams = [None]\n",
    "    hyperparams = [1, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 225]\n",
    "else: hyperparams = [10] # input best parameter for test\n",
    "\n",
    "# results array format: [[train], [validation], [log loss]]\n",
    "results = [[],[],[]]\n",
    "\n",
    "# calculate train error, test error, log loss & time for specific param\n",
    "for n_comp in components_to_test:\n",
    "    for penalty in hyperparams:\n",
    "\n",
    "        if (ENABLE_KFOLD): train_res, val_res, log_loss_val, time_val = kFoldValidation(n_comp, penalty, 10)\n",
    "        elif (ENABLE_LOO): train_res, val_res, log_loss_val, time_val = looValidation(n_comp, penalty)\n",
    "\n",
    "        # save error, RMSE, log loss for each penalty for graph\n",
    "        if (not ENABLE_REGRESSION_TARGET):\n",
    "            results[0].append(1-train_res)\n",
    "            results[1].append(1-val_res)\n",
    "            results[2].append(log_loss_val)\n",
    "        elif (ENABLE_REGRESSION_TARGET):\n",
    "            results[0].append(train_res)\n",
    "            results[1].append(val_res)\n",
    "\n",
    "#         print (\"-----C={}-----\".format(penalty))\n",
    "#         print (\"Time: {} seconds\".format(time_val))\n",
    "#         print (\"-----Train-----\")\n",
    "#         print (\"Accuracy/RMSE: {}\".format(train_res))\n",
    "#         print (\"-----Validation-----\")\n",
    "#         print (\"Accuracy/RMSE: {}\".format(val_res))\n",
    "#         print (\"Log Loss: {}\\n\".format(log_loss_val))\n",
    "        all_results = all_results.append(pd.DataFrame([[penalty, time_val, val_res, log_loss_val]], columns=cols))\n",
    "    \n",
    "# run best model on unseen test set\n",
    "if(ENABLE_TEST):\n",
    "    test_accuracy, test_log_loss, test_time = runTestSet(X, Y, X_tst, Y_tst, n_comp, hyperparams[0])\n",
    "\n",
    "    print (\"-----Test-----\")\n",
    "    print (\"Accuracy/RMSE: {}\".format(test_accuracy))\n",
    "    print (\"Log Loss: {}\".format(test_log_loss))\n",
    "    print (\"Time: {} seconds\".format(test_time))\n",
    "\n",
    "\"\"\"\n",
    "# create error and log loss graph for penalty iterations - classification only\n",
    "if(not ENABLE_REGRESSION_TARGET and len(hyperparams) > 1):\n",
    "    f, axarr = plt.subplots(2, sharex=False)\n",
    "    f.suptitle('Error and Log Loss', y = 0.92)\n",
    "    f.set_size_inches(10, 10)\n",
    "\n",
    "    # subplot 1: error plot\n",
    "    axarr[0].set_ylabel('Error')\n",
    "    axarr[0].plot(hyperparams, results[0], color='r', label='train')\n",
    "    axarr[0].plot(hyperparams, results[1], color='b', label='validation')\n",
    "    axarr[0].set_xticks(hyperparams)\n",
    "    axarr[0].legend()\n",
    "\n",
    "    # subplot 2: log loss plot\n",
    "    axarr[1].set_ylabel('Log Loss')\n",
    "    axarr[1].plot(hyperparams, results[2], color='g', label='log loss')\n",
    "    axarr[1].set_xticks(hyperparams)\n",
    "    axarr[1].set_xlabel('Parameter')\n",
    "    plt.show()\n",
    "    \n",
    "# create RMSE graph for penalty iterations - regression only\n",
    "if(ENABLE_REGRESSION_TARGET and len(hyperparams) > 1):\n",
    "    f, axarr = plt.subplots(sharex=False)\n",
    "    f.suptitle('RMSE', y = 0.92)\n",
    "    f.set_size_inches(10, 5)\n",
    "\n",
    "    # subplot 1: RMSE plot\n",
    "    axarr.set_ylabel('RMSE')\n",
    "    axarr.plot(hyperparams, results[0], color='r', label='train')\n",
    "    axarr.plot(hyperparams, results[1], color='b', label='validation')\n",
    "    axarr.set_xticks(hyperparams)\n",
    "    axarr.set_xlabel('Parameter')\n",
    "    axarr.legend()\n",
    "    plt.show()\n",
    "\"\"\"\n",
    "pd.options.display.max_rows = 999\n",
    "all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
