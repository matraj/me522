{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYDE 522 Project Code\n",
    "Chang Li, Maathusan Rajendram, Anastasia Santasheva, Evan Yeung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard useful packages \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# validation & normalization methods\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, LeaveOneOut\n",
    "\n",
    "# accuracy, MSE, log loss & timer methods\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import log_loss\n",
    "from time import time\n",
    "\n",
    "# dim reduction & classification methods \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "\n",
    "# make matplotlib to show plots inline\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configuration\n",
    "* Select options for method validation\n",
    "* Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. set dataset\n",
    "ENABLE_POR_DATA = True     # set Portugese course dataset\n",
    "ENABLE_MAT_DATA = False    # set Math course dataset\n",
    "\n",
    "# 2. set input setup\n",
    "ENABLE_INPUT_SETUP_B = False   # adds the delta of G1 and G2 as a new col (GDelta)\n",
    "\n",
    "# 3. set supervised approach for G3\n",
    "ENABLE_BINARY_TARGET = False       # sets G3 to binary\n",
    "ENABLE_5LEVEL_TARGET = False        # set G3 to five-level scale\n",
    "ENABLE_REGRESSION_TARGET = True   # set G3 to current state for regression\n",
    "\n",
    "# 4. set dimensionaltiy reduction method - set both to false for none\n",
    "ENABLE_PCN = False\n",
    "ENABLE_LDA = True\n",
    "\n",
    "# 5. set validation type\n",
    "ENABLE_KFOLD = True\n",
    "ENABLE_LOO = False\n",
    "\n",
    "# 6. set final test\n",
    "ENABLE_TEST = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load  Dataset\n",
    "* Select a data set (Portugese course or Math course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((519, 33), (130, 33))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data from csv\n",
    "if (ENABLE_POR_DATA):\n",
    "    dataframe = pd.read_csv('student-por-train.csv', usecols = range(0,33)) \n",
    "    dataframe_test = pd.read_csv('student-por-test.csv', usecols = range(0,33)) \n",
    "elif (ENABLE_MAT_DATA): \n",
    "    dataframe = pd.read_csv('student-mat-train.csv', usecols = range(0,33))\n",
    "    dataframe_test = pd.read_csv('student-mat-test.csv', usecols = range(0,33))\n",
    "\n",
    "dataset = dataframe.values\n",
    "dataset_test = dataframe_test.values\n",
    "dataset.shape, dataset_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "* Convert nominal attributes with Integer + One-Hot Encoding\n",
    "* Selects supervised approache for G3\n",
    "* NOTE: if we want we can also split further into A,B,C (A= all cols, B=same as A without G2, C=same as B without G1)\n",
    "    * But leaving this out for now since we know A gives best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# helper functions for preprocessing\n",
    "def convertToBinary(df, num_cols):\n",
    "    df.loc[(df.G3 < 10), 'G3'] = 0\n",
    "    df.loc[(df.G3 >= 10), 'G3'] = 1\n",
    "    \n",
    "    G3 = df['G3'].values\n",
    "    return G3\n",
    "\n",
    "def convertToFiveLevel(df, num_cols):\n",
    "    df.loc[(df.G3 <= 9), 'G3'] = 0\n",
    "    df.loc[(df.G3 > 9) & (df.G3 <= 11), 'G3'] = 1\n",
    "    df.loc[(df.G3 > 11) & (df.G3 <= 13), 'G3'] = 2\n",
    "    df.loc[(df.G3 > 13) & (df.G3 <= 16), 'G3'] = 3\n",
    "    df.loc[(df.G3 > 16), 'G3'] = 4\n",
    "    \n",
    "    G3 = df['G3'].values\n",
    "    return G3   \n",
    "\n",
    "def convertToInputSetupB(df, num_cols):\n",
    "    df.insert(num_cols-1, 'GDelta', df.G2 - df.G1, allow_duplicates=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def oneHotEncode(df):\n",
    "    df = df.drop(labels='G3', axis=1)\n",
    "    cols_to_transform = [\n",
    "                        'school',\n",
    "                        'sex',\n",
    "                        'address',\n",
    "                        'famsize',\n",
    "                        'Pstatus',\n",
    "                        'Mjob',                        \n",
    "                        'Fjob',\n",
    "                        'reason',\n",
    "                        'guardian',\n",
    "                        'famsup',\n",
    "                        'schoolsup',\n",
    "                        'paid',\n",
    "                        'activities',\n",
    "                        'nursery',                        \n",
    "                        'higher',\n",
    "                        'internet',\n",
    "                        'romantic',\n",
    "                        ]\n",
    "    hot_encoded_df = pd.get_dummies(df, columns = cols_to_transform)\n",
    "    \n",
    "    attributes = hot_encoded_df.values\n",
    "    return attributes\n",
    "\n",
    "def normalizeData(train_data, val_data):\n",
    "    scaler = StandardScaler().fit(train_data)\n",
    "    train_data = scaler.transform(train_data)\n",
    "    val_data = scaler.transform(val_data)\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "# labels of Y\n",
    "labels = []\n",
    "\n",
    "# switch to input setup B\n",
    "if (ENABLE_INPUT_SETUP_B): \n",
    "    dataframe = convertToInputSetupB(dataframe, num_cols)\n",
    "    dataframe_test = convertToInputSetupB(dataframe_test, num_cols)\n",
    "\n",
    "# split one-hot encoded attributes (X) and G3 (Y)\n",
    "X = oneHotEncode(dataframe)\n",
    "X_tst = oneHotEncode(dataframe_test)\n",
    "\n",
    "# selects supervised approach for G3\n",
    "if (ENABLE_BINARY_TARGET):\n",
    "    Y = convertToBinary(dataframe, num_cols).astype('int')\n",
    "    Y_tst = convertToBinary(dataframe_test, num_cols).astype('int')\n",
    "    labels = [0, 1]\n",
    "elif (ENABLE_5LEVEL_TARGET):\n",
    "    Y = convertToFiveLevel(dataframe, num_cols).astype('int')\n",
    "    Y_tst = convertToFiveLevel(dataframe_test, num_cols).astype('int')\n",
    "    labels = [0, 1, 2, 3, 4]\n",
    "elif (ENABLE_REGRESSION_TARGET):\n",
    "    Y = dataframe['G3'].values.astype('int')\n",
    "    Y_tst = dataframe_test['G3'].values.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((519, 58), (519,), (130, 58), (130,))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y\n",
    "X.shape, Y.shape, X_tst.shape, Y_tst.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "* PCA & LDA reduction methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcaReduction(train_data, val_data, n_comp):\n",
    "    pca = PCA(n_components=n_comp)\n",
    "    train_data = pca.fit_transform(train_data)\n",
    "    val_data = pca.transform(val_data)\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "def ldaReduction(train_data, train_target, val_data, n_comp):\n",
    "    lda = LinearDiscriminantAnalysis(n_components=n_comp)\n",
    "    train_data = lda.fit_transform(train_data, train_target)\n",
    "    val_data = lda.transform(val_data)\n",
    "    \n",
    "    return train_data, val_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Methods\n",
    "* k-Fold cross validation & Leave-one-out validation\n",
    "* data gets normalized here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to calculate accuracy (PCC) & RMSE\n",
    "def calcMetric(actual, predicted):\n",
    "    if(ENABLE_REGRESSION_TARGET): return (mean_squared_error(actual, predicted))**(0.5) # calculates RMSE\n",
    "    else: return accuracy_score(actual, predicted, normalize = True) # calculates PCC\n",
    "\n",
    "def kFoldValidation(n_comp, penalty, n_splits=10):\n",
    "    kFold = KFold(n_splits=n_splits)\n",
    "    \n",
    "    # run on validation data\n",
    "    val_results = []\n",
    "    train_results = []\n",
    "    log_loss_results = []\n",
    "    time_log = []\n",
    "    \n",
    "    for train_index, val_index in kFold.split(X):\n",
    "        #start timer, return avg time below\n",
    "        start = time()\n",
    "        \n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        Y_train, Y_val = Y[train_index], Y[val_index]\n",
    "        \n",
    "        # normalize data\n",
    "        X_train, X_val = normalizeData(X_train, X_val)\n",
    "        \n",
    "        # reduce dimensionality\n",
    "        if (ENABLE_PCN): X_train, X_val = pcaReduction(X_train, X_val, n_comp)\n",
    "        elif (ENABLE_LDA): X_train, X_val = ldaReduction(X_train, Y_train, X_val, n_comp)\n",
    "        \n",
    "        # build classifier for each set\n",
    "        clf = buildClf(X_train, Y_train, penalty)\n",
    "        \n",
    "        predicted = clf.predict(X_val)\n",
    "        val_accuracy = calcMetric(Y_val, predicted)\n",
    "        predicted = clf.predict(X_train)\n",
    "        train_accuracy = calcMetric(Y_train, predicted)\n",
    "        \n",
    "        # log loss calculation - for classification only\n",
    "        if(not ENABLE_REGRESSION_TARGET): log_loss_results.append(log_loss(Y_val, clf.predict_proba(X_val), labels=labels))\n",
    "        \n",
    "        val_results.append(val_accuracy)\n",
    "        train_results.append(train_accuracy)\n",
    "        time_log.append(time()-start)\n",
    "        \n",
    "    return np.mean(train_results, axis = 0), np.mean(val_results, axis = 0), np.mean(log_loss_results, axis = 0), np.mean(time_log, axis = 0)\n",
    "\n",
    "def looValidation(n_comp, penalty):\n",
    "    loo = LeaveOneOut()\n",
    "    \n",
    "    # run on validation data\n",
    "    val_results = []\n",
    "    train_results = []\n",
    "    Y_test_prob = []\n",
    "    log_loss_value = 0\n",
    "    time_log = []\n",
    "    \n",
    "    for train_index, val_index in loo.split(X):\n",
    "        #start timer, return avg time below\n",
    "        start = time()\n",
    "        \n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        Y_train, Y_val = Y[train_index], Y[val_index]\n",
    "        \n",
    "        # normalize data\n",
    "        X_train, X_val = normalizeData(X_train, X_val)\n",
    "        \n",
    "        # reduce dimensionality\n",
    "        if (ENABLE_PCN): X_train, X_val = pcaReduction(X_train, X_val, n_comp)\n",
    "        elif (ENABLE_LDA): X_train, X_val = ldaReduction(X_train, Y_train, X_val, n_comp)\n",
    "        \n",
    "        # build classifier for each set\n",
    "        clf = buildClf(X_train, Y_train, penalty)\n",
    "        \n",
    "        predicted = clf.predict(X_val)\n",
    "        val_accuracy = calcMetric(Y_val, predicted)\n",
    "        predicted = clf.predict(X_train)\n",
    "        train_accuracy = calcMetric(Y_train, predicted)\n",
    "        \n",
    "        # save probability for log loss calculation\n",
    "        if(not ENABLE_REGRESSION_TARGET): Y_test_prob.append(clf.predict_proba(X_val)[0])\n",
    "            \n",
    "        test_results.append(test_accuracy)\n",
    "        val_results.append(val_accuracy)\n",
    "        train_results.append(train_accuracy)\n",
    "        time_log.append(time()-start)\n",
    "    \n",
    "    # log loss calculation - for classification only\n",
    "    if(not ENABLE_REGRESSION_TARGET): log_loss_value = log_loss(Y, Y_test_prob, labels=labels)\n",
    "\n",
    "    return np.mean(train_results, axis = 0), np.mean(val_results, axis = 0), log_loss_value, np.mean(time_log, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Selection\n",
    "* Select classifer to validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVotingClassifier():\n",
    "    clf1 = SVC(C=10000, probability=True, kernel='rbf', gamma=0.0001)\n",
    "    clf2 = RandomForestClassifier(n_estimators = 100, criterion = \"entropy\")\n",
    "    clf3 = GaussianNB()\n",
    "\n",
    "    return VotingClassifier(estimators=[('svm', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')\n",
    "\n",
    "# method to build classifier\n",
    "def buildClf(train_data, train_target, penalty):\n",
    "    \n",
    "    if(ENABLE_REGRESSION_TARGET):\n",
    "#         model = SVR(C=0.0001, kernel='sigmoid', gamma=penalty) # for regression\n",
    "        model = KNeighborsClassifier(n_neighbors=penalty)\n",
    "#         model = SVR(C=penalty, kernel='linear') # for regression\n",
    "    else:\n",
    "#         model = SVC(C=penalty, probability=True, kernel='linear')\n",
    "#         model = getVotingClassifier()\n",
    "#         model = GaussianNB()\n",
    "#         model = MLPClassifier(solver = 'lbfgs')\n",
    "#         model = DecisionTreeClassifier(max_depth = None, max_features = penalty, criterion = \"entropy\")\n",
    "#         model = RandomForestClassifier(n_estimators = penalty, max_features=450, criterion = \"entropy\")\n",
    "        model = KNeighborsClassifier(n_neighbors=penalty)\n",
    "#         model = BaggingClassifier(DecisionTreeClassifier(max_features = penalty, criterion = \"entropy\"), max_samples=0.5, max_features=6)\n",
    "\n",
    "    model.fit(train_data, train_target)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Test Method\n",
    "* Run best classifier on unseen test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get accuracy/RMSE, logloss and time \n",
    "def runTestSet(X_train, Y_train, X_test, Y_test, n_comp, penalty):\n",
    "    \n",
    "    start = time()\n",
    "    test_log_loss = 0\n",
    "    \n",
    "    # normalize data\n",
    "    X_train, X_test = normalizeData(X_train, X_test)\n",
    "\n",
    "    # reduce dimensionality\n",
    "    if (ENABLE_PCN): X_train, X_test = pcaReduction(X_train, X_test, n_comp)\n",
    "    elif (ENABLE_LDA): X_train, X_test = ldaReduction(X_train, Y_train, X_test, n_comp)\n",
    "\n",
    "    # build classifier, predict and get accuracy\n",
    "    clf = buildClf(X_train, Y_train, penalty)\n",
    "    predicted = clf.predict(X_test)\n",
    "    test_accuracy = calcMetric(Y_test, predicted)\n",
    "    \n",
    "    # log loss calculation - for classification only\n",
    "    if(not ENABLE_REGRESSION_TARGET): test_log_loss = log_loss(Y_test, clf.predict_proba(X_test))\n",
    "        \n",
    "    # execution time calculation\n",
    "    test_time = time()-start\n",
    "    \n",
    "    return test_accuracy, test_log_loss, test_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain Accuracy, Log Loss & Error\n",
    "* Adjust array of penalty parameters\n",
    "* Graphs error and log loss - for classification only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/usr/local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hparam</th>\n",
       "      <th>Time</th>\n",
       "      <th>Acc</th>\n",
       "      <th>logloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.015495</td>\n",
       "      <td>1.906583</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.015850</td>\n",
       "      <td>1.448257</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0.014175</td>\n",
       "      <td>1.411135</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.016368</td>\n",
       "      <td>1.564560</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>0.013608</td>\n",
       "      <td>1.545906</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0.014504</td>\n",
       "      <td>1.554759</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>0.020861</td>\n",
       "      <td>1.607539</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.031944</td>\n",
       "      <td>1.832035</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125</td>\n",
       "      <td>0.022044</td>\n",
       "      <td>1.918038</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150</td>\n",
       "      <td>0.070742</td>\n",
       "      <td>1.999575</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>175</td>\n",
       "      <td>0.038060</td>\n",
       "      <td>2.214796</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>0.046379</td>\n",
       "      <td>2.371796</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>225</td>\n",
       "      <td>0.065887</td>\n",
       "      <td>2.451234</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.025161</td>\n",
       "      <td>1.850803</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.023480</td>\n",
       "      <td>1.491402</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0.012621</td>\n",
       "      <td>1.507134</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.014023</td>\n",
       "      <td>1.561530</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>0.014553</td>\n",
       "      <td>1.576984</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0.016355</td>\n",
       "      <td>1.603944</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>0.017978</td>\n",
       "      <td>1.671585</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.030302</td>\n",
       "      <td>1.911305</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125</td>\n",
       "      <td>0.022085</td>\n",
       "      <td>1.937324</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150</td>\n",
       "      <td>0.025103</td>\n",
       "      <td>1.996665</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>175</td>\n",
       "      <td>0.026305</td>\n",
       "      <td>2.234638</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>2.342127</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>225</td>\n",
       "      <td>0.035501</td>\n",
       "      <td>2.452424</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.011346</td>\n",
       "      <td>1.766411</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.011369</td>\n",
       "      <td>1.438576</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0.012682</td>\n",
       "      <td>1.433733</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.013517</td>\n",
       "      <td>1.576585</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>0.014392</td>\n",
       "      <td>1.627063</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0.015784</td>\n",
       "      <td>1.645518</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>0.017514</td>\n",
       "      <td>1.753693</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.020938</td>\n",
       "      <td>1.941609</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125</td>\n",
       "      <td>0.022236</td>\n",
       "      <td>1.960544</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150</td>\n",
       "      <td>0.025492</td>\n",
       "      <td>2.043436</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>175</td>\n",
       "      <td>0.027478</td>\n",
       "      <td>2.247867</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>0.031105</td>\n",
       "      <td>2.369724</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>225</td>\n",
       "      <td>0.032701</td>\n",
       "      <td>2.466856</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.010134</td>\n",
       "      <td>1.570353</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.012268</td>\n",
       "      <td>1.575119</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0.013796</td>\n",
       "      <td>1.512735</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.014351</td>\n",
       "      <td>1.584114</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>0.015870</td>\n",
       "      <td>1.693710</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0.016535</td>\n",
       "      <td>1.693028</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>0.018180</td>\n",
       "      <td>1.832669</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.021349</td>\n",
       "      <td>1.948500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125</td>\n",
       "      <td>0.023177</td>\n",
       "      <td>1.974010</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150</td>\n",
       "      <td>0.026794</td>\n",
       "      <td>2.055230</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>175</td>\n",
       "      <td>0.028570</td>\n",
       "      <td>2.263046</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>0.030638</td>\n",
       "      <td>2.372098</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>225</td>\n",
       "      <td>0.037373</td>\n",
       "      <td>2.459343</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  hparam      Time       Acc  logloss\n",
       "0      1  0.015495  1.906583      NaN\n",
       "0     10  0.015850  1.448257      NaN\n",
       "0     20  0.014175  1.411135      NaN\n",
       "0     30  0.016368  1.564560      NaN\n",
       "0     40  0.013608  1.545906      NaN\n",
       "0     50  0.014504  1.554759      NaN\n",
       "0     75  0.020861  1.607539      NaN\n",
       "0    100  0.031944  1.832035      NaN\n",
       "0    125  0.022044  1.918038      NaN\n",
       "0    150  0.070742  1.999575      NaN\n",
       "0    175  0.038060  2.214796      NaN\n",
       "0    200  0.046379  2.371796      NaN\n",
       "0    225  0.065887  2.451234      NaN\n",
       "0      1  0.025161  1.850803      NaN\n",
       "0     10  0.023480  1.491402      NaN\n",
       "0     20  0.012621  1.507134      NaN\n",
       "0     30  0.014023  1.561530      NaN\n",
       "0     40  0.014553  1.576984      NaN\n",
       "0     50  0.016355  1.603944      NaN\n",
       "0     75  0.017978  1.671585      NaN\n",
       "0    100  0.030302  1.911305      NaN\n",
       "0    125  0.022085  1.937324      NaN\n",
       "0    150  0.025103  1.996665      NaN\n",
       "0    175  0.026305  2.234638      NaN\n",
       "0    200  0.030273  2.342127      NaN\n",
       "0    225  0.035501  2.452424      NaN\n",
       "0      1  0.011346  1.766411      NaN\n",
       "0     10  0.011369  1.438576      NaN\n",
       "0     20  0.012682  1.433733      NaN\n",
       "0     30  0.013517  1.576585      NaN\n",
       "0     40  0.014392  1.627063      NaN\n",
       "0     50  0.015784  1.645518      NaN\n",
       "0     75  0.017514  1.753693      NaN\n",
       "0    100  0.020938  1.941609      NaN\n",
       "0    125  0.022236  1.960544      NaN\n",
       "0    150  0.025492  2.043436      NaN\n",
       "0    175  0.027478  2.247867      NaN\n",
       "0    200  0.031105  2.369724      NaN\n",
       "0    225  0.032701  2.466856      NaN\n",
       "0      1  0.010134  1.570353      NaN\n",
       "0     10  0.012268  1.575119      NaN\n",
       "0     20  0.013796  1.512735      NaN\n",
       "0     30  0.014351  1.584114      NaN\n",
       "0     40  0.015870  1.693710      NaN\n",
       "0     50  0.016535  1.693028      NaN\n",
       "0     75  0.018180  1.832669      NaN\n",
       "0    100  0.021349  1.948500      NaN\n",
       "0    125  0.023177  1.974010      NaN\n",
       "0    150  0.026794  2.055230      NaN\n",
       "0    175  0.028570  2.263046      NaN\n",
       "0    200  0.030638  2.372098      NaN\n",
       "0    225  0.037373  2.459343      NaN"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['hparam', 'Time', 'Acc', 'logloss']\n",
    "all_results = pd.DataFrame(columns=cols)\n",
    "\n",
    "# input reduced dimension - this can be ignored if none selected\n",
    "components_to_test = [1, 2, 3, 4]\n",
    "\n",
    "# input parameter iterations - can tune other params in classifer selection method above\n",
    "if(not ENABLE_TEST):\n",
    "    # hyperparams = [None]\n",
    "    hyperparams = [1, 10, 20, 30, 40, 50, 75, 100, 125, 150, 175, 200, 225]\n",
    "else: hyperparams = [10] # input best parameter for test\n",
    "\n",
    "# results array format: [[train], [validation], [log loss]]\n",
    "results = [[],[],[]]\n",
    "\n",
    "# calculate train error, test error, log loss & time for specific param\n",
    "for n_comp in components_to_test:\n",
    "    for penalty in hyperparams:\n",
    "\n",
    "        if (ENABLE_KFOLD): train_res, val_res, log_loss_val, time_val = kFoldValidation(n_comp, penalty, 10)\n",
    "        elif (ENABLE_LOO): train_res, val_res, log_loss_val, time_val = looValidation(n_comp, penalty)\n",
    "\n",
    "        # save error, RMSE, log loss for each penalty for graph\n",
    "        if (not ENABLE_REGRESSION_TARGET):\n",
    "            results[0].append(1-train_res)\n",
    "            results[1].append(1-val_res)\n",
    "            results[2].append(log_loss_val)\n",
    "        elif (ENABLE_REGRESSION_TARGET):\n",
    "            results[0].append(train_res)\n",
    "            results[1].append(val_res)\n",
    "\n",
    "#         print (\"-----C={}-----\".format(penalty))\n",
    "#         print (\"Time: {} seconds\".format(time_val))\n",
    "#         print (\"-----Train-----\")\n",
    "#         print (\"Accuracy/RMSE: {}\".format(train_res))\n",
    "#         print (\"-----Validation-----\")\n",
    "#         print (\"Accuracy/RMSE: {}\".format(val_res))\n",
    "#         print (\"Log Loss: {}\\n\".format(log_loss_val))\n",
    "        all_results = all_results.append(pd.DataFrame([[penalty, time_val, val_res, log_loss_val]], columns=cols))\n",
    "    \n",
    "# run best model on unseen test set\n",
    "if(ENABLE_TEST):\n",
    "    test_accuracy, test_log_loss, test_time = runTestSet(X, Y, X_tst, Y_tst, n_comp, hyperparams[0])\n",
    "\n",
    "    print (\"-----Test-----\")\n",
    "    print (\"Accuracy/RMSE: {}\".format(test_accuracy))\n",
    "    print (\"Log Loss: {}\".format(test_log_loss))\n",
    "    print (\"Time: {} seconds\".format(test_time))\n",
    "\n",
    "\"\"\"\n",
    "# create error and log loss graph for penalty iterations - classification only\n",
    "if(not ENABLE_REGRESSION_TARGET and len(hyperparams) > 1):\n",
    "    f, axarr = plt.subplots(2, sharex=False)\n",
    "    f.suptitle('Error and Log Loss', y = 0.92)\n",
    "    f.set_size_inches(10, 10)\n",
    "\n",
    "    # subplot 1: error plot\n",
    "    axarr[0].set_ylabel('Error')\n",
    "    axarr[0].plot(hyperparams, results[0], color='r', label='train')\n",
    "    axarr[0].plot(hyperparams, results[1], color='b', label='validation')\n",
    "    axarr[0].set_xticks(hyperparams)\n",
    "    axarr[0].legend()\n",
    "\n",
    "    # subplot 2: log loss plot\n",
    "    axarr[1].set_ylabel('Log Loss')\n",
    "    axarr[1].plot(hyperparams, results[2], color='g', label='log loss')\n",
    "    axarr[1].set_xticks(hyperparams)\n",
    "    axarr[1].set_xlabel('Parameter')\n",
    "    plt.show()\n",
    "    \n",
    "# create RMSE graph for penalty iterations - regression only\n",
    "if(ENABLE_REGRESSION_TARGET and len(hyperparams) > 1):\n",
    "    f, axarr = plt.subplots(sharex=False)\n",
    "    f.suptitle('RMSE', y = 0.92)\n",
    "    f.set_size_inches(10, 5)\n",
    "\n",
    "    # subplot 1: RMSE plot\n",
    "    axarr.set_ylabel('RMSE')\n",
    "    axarr.plot(hyperparams, results[0], color='r', label='train')\n",
    "    axarr.plot(hyperparams, results[1], color='b', label='validation')\n",
    "    axarr.set_xticks(hyperparams)\n",
    "    axarr.set_xlabel('Parameter')\n",
    "    axarr.legend()\n",
    "    plt.show()\n",
    "\"\"\"\n",
    "pd.options.display.max_rows = 999\n",
    "all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
