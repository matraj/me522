{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYDE 522 Project Code\n",
    "Chang Li, Maathusan Rajendram, Anastasia Santasheva, Evan Yeung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard useful packages \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# validation & normalization methods\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, LeaveOneOut\n",
    "\n",
    "# accuracy, MSE, log loss & timer methods\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import log_loss\n",
    "from time import time\n",
    "\n",
    "# dim reduction & classification methods \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "\n",
    "# GA stuff\n",
    "import random\n",
    "from deap import base, creator, tools\n",
    "\n",
    "# make matplotlib to show plots inline\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configuration\n",
    "* Select options for method validation\n",
    "* Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. set dataset\n",
    "ENABLE_POR_DATA = False     # set Portugese course dataset\n",
    "ENABLE_MAT_DATA = True    # set Math course dataset\n",
    "\n",
    "# 2. set input setup\n",
    "ENABLE_INPUT_SETUP_B = False   # adds the delta of G1 and G2 as a new col (GDelta)\n",
    "\n",
    "# 3. set supervised approach for G3\n",
    "ENABLE_BINARY_TARGET = True       # sets G3 to binary\n",
    "ENABLE_5LEVEL_TARGET = False        # set G3 to five-level scale\n",
    "ENABLE_REGRESSION_TARGET = False   # set G3 to current state for regression\n",
    "\n",
    "# 4. set dimensionaltiy reduction method - set both to false for none\n",
    "ENABLE_PCN = False\n",
    "ENABLE_LDA = False\n",
    "\n",
    "# 5. set validation type\n",
    "ENABLE_KFOLD = True\n",
    "ENABLE_LOO = False\n",
    "\n",
    "# 6. set final test\n",
    "ENABLE_TEST = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load  Dataset\n",
    "* Select a data set (Portugese course or Math course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data from csv\n",
    "if (ENABLE_POR_DATA):\n",
    "    dataframe = pd.read_csv('student-por-train.csv', usecols = range(0,33)) \n",
    "    dataframe_test = pd.read_csv('student-por-test.csv', usecols = range(0,33)) \n",
    "elif (ENABLE_MAT_DATA): \n",
    "    dataframe = pd.read_csv('student-mat-train.csv', usecols = range(0,33))\n",
    "    dataframe_test = pd.read_csv('student-mat-test.csv', usecols = range(0,33))\n",
    "\n",
    "# shuffle dataset\n",
    "dataframe = dataframe.sample(frac=1)\n",
    "\n",
    "# find col length\n",
    "num_cols = dataframe.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "* Convert nominal attributes with Integer + One-Hot Encoding\n",
    "* Selects supervised approache for G3\n",
    "* NOTE: if we want we can also split further into A,B,C (A= all cols, B=same as A without G2, C=same as B without G1)\n",
    "    * But leaving this out for now since we know A gives best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# helper functions for preprocessing\n",
    "def convertToBinary(df, num_cols):\n",
    "    df.loc[(df.G3 < 10), 'G3'] = 0\n",
    "    df.loc[(df.G3 >= 10), 'G3'] = 1\n",
    "    \n",
    "    G3 = df['G3'].values\n",
    "    return G3\n",
    "\n",
    "def convertToFiveLevel(df, num_cols):\n",
    "    df.loc[(df.G3 <= 9), 'G3'] = 0\n",
    "    df.loc[(df.G3 > 9) & (df.G3 <= 11), 'G3'] = 1\n",
    "    df.loc[(df.G3 > 11) & (df.G3 <= 13), 'G3'] = 2\n",
    "    df.loc[(df.G3 > 13) & (df.G3 <= 16), 'G3'] = 3\n",
    "    df.loc[(df.G3 > 16), 'G3'] = 4\n",
    "    \n",
    "    G3 = df['G3'].values\n",
    "    return G3   \n",
    "\n",
    "def convertToInputSetupB(df, num_cols):\n",
    "    df.insert(num_cols-1, 'GDelta', df.G2 - df.G1, allow_duplicates=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def oneHotEncode(df):\n",
    "    df = df.drop(labels='G3', axis=1)\n",
    "    cols_to_transform = [\n",
    "                        'school',\n",
    "                        'sex',\n",
    "                        'address',\n",
    "                        'famsize',\n",
    "                        'Pstatus',\n",
    "                        'Mjob',                        \n",
    "                        'Fjob',\n",
    "                        'reason',\n",
    "                        'guardian',\n",
    "                        'famsup',\n",
    "                        'schoolsup',\n",
    "                        'paid',\n",
    "                        'activities',\n",
    "                        'nursery',                        \n",
    "                        'higher',\n",
    "                        'internet',\n",
    "                        'romantic',\n",
    "                        ]\n",
    "    hot_encoded_df = pd.get_dummies(df, columns = cols_to_transform)\n",
    "    \n",
    "    attributes = hot_encoded_df.values\n",
    "    return attributes\n",
    "\n",
    "def normalizeData(train_data, val_data):\n",
    "    scaler = StandardScaler().fit(train_data)\n",
    "    train_data = scaler.transform(train_data)\n",
    "    val_data = scaler.transform(val_data)\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "# labels of Y\n",
    "labels = []\n",
    "\n",
    "# switch to input setup B\n",
    "if (ENABLE_INPUT_SETUP_B): \n",
    "    dataframe = convertToInputSetupB(dataframe, num_cols)\n",
    "    dataframe_test = convertToInputSetupB(dataframe_test, num_cols)\n",
    "\n",
    "# split one-hot encoded attributes (X) and G3 (Y)\n",
    "X = oneHotEncode(dataframe)\n",
    "X_tst = oneHotEncode(dataframe_test)\n",
    "\n",
    "# selects supervised approach for G3\n",
    "if (ENABLE_BINARY_TARGET):\n",
    "    Y = convertToBinary(dataframe, num_cols).astype('int')\n",
    "    Y_tst = convertToBinary(dataframe_test, num_cols).astype('int')\n",
    "    labels = [0, 1]\n",
    "elif (ENABLE_5LEVEL_TARGET):\n",
    "    Y = convertToFiveLevel(dataframe, num_cols).astype('int')\n",
    "    Y_tst = convertToFiveLevel(dataframe_test, num_cols).astype('int')\n",
    "    labels = [0, 1, 2, 3, 4]\n",
    "elif (ENABLE_REGRESSION_TARGET):\n",
    "    Y = dataframe['G3'].values.astype('int')\n",
    "    Y_tst = dataframe_test['G3'].values.astype('int')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((316, 58), (316,), (79, 58), (79,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y\n",
    "X.shape, Y.shape, X_tst.shape, Y_tst.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "* PCA & LDA reduction methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcaReduction(train_data, val_data, n_comp):\n",
    "    pca = PCA(n_components=n_comp)\n",
    "    train_data = pca.fit_transform(train_data)\n",
    "    val_data = pca.transform(val_data)\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "def ldaReduction(train_data, train_target, val_data, n_comp):\n",
    "    lda = LinearDiscriminantAnalysis(n_components=n_comp)\n",
    "    train_data = lda.fit_transform(train_data, train_target)\n",
    "    val_data = lda.transform(val_data)\n",
    "    \n",
    "    return train_data, val_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make NN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First parameter is the number of hidden layers\n",
    "# Remaining parameters are number of nodes in that layer\n",
    "def createNNIndividual(params):\n",
    "    num_hidden_layers = params[0] - 1\n",
    "    hidden_layers = tuple(params[1:num_hidden_layers + 1])\n",
    "    model = MLPClassifier(solver='lbfgs', hidden_layer_sizes=tuple(params))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Selection\n",
    "* Select classifer to validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to build classifier\n",
    "def buildClf(train_data, train_target, params):\n",
    "    if(ENABLE_REGRESSION_TARGET):\n",
    "        pass # will cause an error\n",
    "    else:\n",
    "        # built nn\n",
    "        model = createNNIndividual(params)\n",
    "\n",
    "    model.fit(train_data, train_target)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and run GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because of the way this is encoded, it works better if they're the same :/\n",
    "MAX_LAYERS = 10\n",
    "MAX_NODES_PER_LAYER = 10\n",
    "\n",
    "# Add creators\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "# Build toolbox\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_int\", random.randint, 1, MAX_NODES_PER_LAYER)\n",
    "# add 2 as first int is the num_layers - 1\n",
    "toolbox.register(\"individual\", tools.initRepeat,\n",
    "                 creator.Individual, toolbox.attr_int, MAX_LAYERS + 2)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to calculate accuracy (PCC) & RMSE\n",
    "def calcMetric(actual, predicted):\n",
    "    if(ENABLE_REGRESSION_TARGET): return (mean_squared_error(actual, predicted))**(0.5) # calculates RMSE\n",
    "    else: return accuracy_score(actual, predicted, normalize = True) # calculates PCC\n",
    "\n",
    "\n",
    "def evaluateIndividual(nn_params):\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "    # normalize data\n",
    "    X_train, X_val = normalizeData(X_train, X_val)\n",
    "\n",
    "    # reduce dimensionality\n",
    "    if (ENABLE_PCN): X_train, X_val = pcaReduction(X_train, X_val, n_comp)\n",
    "    elif (ENABLE_LDA): X_train, X_val = ldaReduction(X_train, Y_train, X_val, n_comp)\n",
    "\n",
    "    # build classifier for each set\n",
    "    clf = buildClf(X_train, Y_train, nn_params)\n",
    "\n",
    "    predicted = clf.predict(X_val)\n",
    "    val_accuracy = calcMetric(Y_val, predicted)\n",
    "        \n",
    "    return val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolbox.register(\"evaluate\", evaluateIndividual)\n",
    "toolbox.register(\"mate\", tools.cxPartialyMatched)\n",
    "toolbox.register(\"mutate\", tools.mutUniformInt, low=1, up=MAX_NODES_PER_LAYER, indpb=0.05)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evan/.virtualenvs/machine_intelligence/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gen   min_val   max_val       avg       std\n",
      "0    1  0.578125  0.921875  0.689063  0.073769\n",
      "   gen  min_val  max_val       avg       std\n",
      "0    2  0.53125    0.875  0.705078  0.088746\n",
      "   gen   min_val   max_val       avg      std\n",
      "0    3  0.546875  0.890625  0.748047  0.09212\n",
      "   gen  min_val  max_val       avg       std\n",
      "0    4  0.53125  0.90625  0.703906  0.099311\n",
      "   gen   min_val  max_val       avg       std\n",
      "0    5  0.546875  0.90625  0.710938  0.118482\n",
      "   gen   min_val  max_val       avg      std\n",
      "0    6  0.515625  0.90625  0.702344  0.09504\n",
      "   gen   min_val  max_val       avg       std\n",
      "0    7  0.546875  0.90625  0.750781  0.111527\n",
      "   gen   min_val   max_val       avg       std\n",
      "0    8  0.546875  0.953125  0.719922  0.108275\n",
      "   gen   min_val   max_val       avg       std\n",
      "0    9  0.546875  0.890625  0.691797  0.082531\n",
      "   gen  min_val   max_val       avg       std\n",
      "0   10  0.53125  0.921875  0.722656  0.096493\n",
      "   gen   min_val   max_val      avg       std\n",
      "0   11  0.578125  0.921875  0.74375  0.101129\n",
      "   gen  min_val   max_val       avg       std\n",
      "0   12  0.59375  0.921875  0.758203  0.093944\n",
      "   gen  min_val  max_val       avg       std\n",
      "0   13  0.59375  0.90625  0.742188  0.099682\n",
      "   gen  min_val  max_val       avg      std\n",
      "0   14  0.53125  0.90625  0.746875  0.11022\n",
      "   gen   min_val  max_val       avg       std\n",
      "0   15  0.546875  0.90625  0.751172  0.109397\n",
      "   gen  min_val   max_val       avg       std\n",
      "0   16  0.53125  0.921875  0.732031  0.101955\n",
      "   gen  min_val  max_val       avg       std\n",
      "0   17  0.53125   0.9375  0.764453  0.111168\n",
      "   gen   min_val  max_val       avg       std\n",
      "0   18  0.578125   0.9375  0.782031  0.097575\n",
      "   gen   min_val   max_val       avg       std\n",
      "0   19  0.546875  0.921875  0.726562  0.108647\n",
      "   gen   min_val   max_val       avg       std\n",
      "0   20  0.578125  0.921875  0.760156  0.098917\n",
      "   gen   min_val   max_val       avg       std\n",
      "0   21  0.578125  0.921875  0.740625  0.108636\n",
      "   gen   min_val   max_val       avg      std\n",
      "0   22  0.546875  0.921875  0.737109  0.11545\n",
      "   gen   min_val  max_val       avg       std\n",
      "0   23  0.578125  0.90625  0.739062  0.111102\n",
      "   gen   min_val  max_val       avg       std\n",
      "0   24  0.546875  0.90625  0.739844  0.106293\n",
      "   gen  min_val   max_val       avg       std\n",
      "0   25  0.59375  0.921875  0.773047  0.098635\n",
      "   gen   min_val   max_val       avg       std\n",
      "0   26  0.578125  0.953125  0.747656  0.119076\n",
      "   gen   min_val  max_val       avg       std\n",
      "0   27  0.578125  0.96875  0.741797  0.103112\n",
      "   gen   min_val  max_val       avg      std\n",
      "0   28  0.515625  0.96875  0.733203  0.11264\n",
      "   gen  min_val  max_val       avg       std\n",
      "0   29   0.5625  0.90625  0.733984  0.098728\n",
      "   gen   min_val   max_val       avg       std\n",
      "0   30  0.515625  0.890625  0.755859  0.098244\n",
      "   gen  min_val  max_val       avg       std\n",
      "0   31  0.53125  0.90625  0.764844  0.104229\n",
      "   gen   min_val  max_val       avg       std\n",
      "0   32  0.546875   0.9375  0.758203  0.098511\n",
      "   gen   min_val  max_val     avg       std\n",
      "0   33  0.515625  0.90625  0.7375  0.109051\n",
      "   gen   min_val   max_val       avg      std\n",
      "0   34  0.546875  0.921875  0.732812  0.10191\n",
      "   gen   min_val   max_val       avg       std\n",
      "0   35  0.546875  0.921875  0.753125  0.098276\n",
      "   gen  min_val  max_val      avg      std\n",
      "0   36  0.53125  0.90625  0.74375  0.09076\n",
      "   gen   min_val  max_val       avg       std\n",
      "0   37  0.578125    0.875  0.735547  0.091469\n",
      "   gen   min_val   max_val       avg       std\n",
      "0   38  0.578125  0.890625  0.748047  0.088746\n",
      "   gen   min_val  max_val       avg       std\n",
      "0   39  0.546875  0.90625  0.728125  0.103279\n",
      "   gen  min_val   max_val       avg       std\n",
      "0   40      0.5  0.921875  0.703906  0.101919\n",
      "   gen  min_val   max_val       avg       std\n",
      "0   41   0.5625  0.921875  0.739453  0.104896\n",
      "   gen  min_val   max_val     avg       std\n",
      "0   42  0.53125  0.921875  0.7375  0.104419\n",
      "   gen  min_val   max_val       avg       std\n",
      "0   43  0.46875  0.921875  0.757422  0.094398\n",
      "   gen  min_val   max_val      avg       std\n",
      "0   44   0.5625  0.921875  0.75625  0.107279\n",
      "   gen   min_val  max_val       avg       std\n",
      "0   45  0.546875  0.90625  0.700391  0.100468\n",
      "   gen  min_val  max_val       avg       std\n",
      "0   46   0.5625  0.90625  0.741797  0.088387\n",
      "   gen  min_val  max_val       avg       std\n",
      "0   47  0.53125  0.90625  0.735938  0.100887\n",
      "   gen   min_val  max_val      avg       std\n",
      "0   48  0.578125  0.90625  0.71875  0.106548\n",
      "   gen   min_val  max_val       avg       std\n",
      "0   49  0.546875   0.9375  0.713281  0.107265\n",
      "   gen  min_val  max_val       avg       std\n",
      "0   50  0.53125  0.90625  0.719922  0.097977\n",
      "   gen   min_val   max_val       avg       std\n",
      "0   51  0.546875  0.890625  0.727344  0.089687\n",
      "   gen   min_val   max_val       avg       std\n",
      "0   52  0.546875  0.890625  0.720313  0.094966\n",
      "   gen   min_val   max_val       avg       std\n",
      "0   53  0.515625  0.921875  0.739453  0.120697\n",
      "   gen   min_val  max_val       avg       std\n",
      "0   54  0.546875  0.90625  0.775391  0.091755\n",
      "   gen   min_val  max_val       avg       std\n",
      "0   55  0.546875    0.875  0.715625  0.106846\n",
      "   gen   min_val  max_val       avg       std\n",
      "0   56  0.546875   0.9375  0.774609  0.096122\n",
      "   gen   min_val  max_val       avg       std\n",
      "0   57  0.546875  0.90625  0.722656  0.102147\n",
      "   gen  min_val   max_val    avg       std\n",
      "0   58   0.5625  0.890625  0.725  0.089062\n",
      "   gen  min_val  max_val       avg       std\n",
      "0   59   0.5625    0.875  0.741797  0.091041\n",
      "   gen   min_val   max_val       avg       std\n",
      "0   60  0.546875  0.890625  0.751563  0.093932\n"
     ]
    }
   ],
   "source": [
    "def mainGA():\n",
    "    POP_SIZE=40\n",
    "    CXPB = 0.6\n",
    "    MUTPB = 0.4\n",
    "    NGEN = 70\n",
    "    \n",
    "    cols = ['gen', 'min_val', 'max_val', 'avg', 'std']\n",
    "    all_results = pd.DataFrame(columns=cols)\n",
    "    all_populations = []\n",
    "\n",
    "    # create population\n",
    "    pop = toolbox.population(n=POP_SIZE)\n",
    "    all_populations.append(list(map(toolbox.clone, pop)))\n",
    "\n",
    "    # Evaluate the entire population\n",
    "    fitnesses = list(map(toolbox.evaluate, pop))\n",
    "    for ind, fit in zip(pop, fitnesses):\n",
    "        ind.fitness.values = (fit,)\n",
    "\n",
    "    # Extracting all the fitnesses of pop\n",
    "    fits = [ind.fitness.values[0] for ind in pop]\n",
    "    \n",
    "    # Variable keeping track of the number of generations\n",
    "    g = 0\n",
    "    \n",
    "    # Begin the evolution\n",
    "    while max(fits) < 100 and g < NGEN:\n",
    "        # A new generation\n",
    "        g = g + 1\n",
    "        \n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(pop, len(pop))\n",
    "        # Clone the selected individuals\n",
    "        offspring = list(map(toolbox.clone, offspring))\n",
    "        \n",
    "        # Apply crossover and mutation on the offspring\n",
    "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "            if random.random() < CXPB:\n",
    "                toolbox.mate(child1, child2)\n",
    "                del child1.fitness.values\n",
    "                del child2.fitness.values\n",
    "\n",
    "        for mutant in offspring:\n",
    "            if random.random() < MUTPB:\n",
    "                toolbox.mutate(mutant)\n",
    "                del mutant.fitness.values\n",
    "                \n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = (fit,)\n",
    "\n",
    "        # The population is entirely replaced by the offspring\n",
    "        pop[:] = offspring\n",
    "        fits = [ind.fitness.values[0] for ind in pop]\n",
    "        \n",
    "        all_populations.append(list(map(toolbox.clone, pop)))\n",
    "        \n",
    "        ## THIS IS JUST FOR METRICS\n",
    "        length = len(pop)\n",
    "        mean = sum(fits) / length\n",
    "        sum2 = sum(x*x for x in fits)\n",
    "        std = abs(sum2 / length - mean**2)**0.5\n",
    "        \n",
    "        generation_results = pd.DataFrame([[g, min(fits), max(fits), mean, std]], columns=cols)\n",
    "        all_results = all_results.append(generation_results)\n",
    "        print(generation_results)\n",
    "\n",
    "    return all_populations, all_results\n",
    "\n",
    "populations, results = mainGA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gens = [i for i in range(results.shape[0])]\n",
    "plt.plot(gens, results.avg, 'b', gens, results.max_val, 'g', gens, results.min_val, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Methods\n",
    "* k-Fold cross validation & Leave-one-out validation\n",
    "* data gets normalized here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFoldValidation(n_comp, params, n_splits=10):\n",
    "    kFold = KFold(n_splits=n_splits)\n",
    "    \n",
    "    # run on validation data\n",
    "    val_results = []\n",
    "    train_results = []\n",
    "    log_loss_results = []\n",
    "    time_log = []\n",
    "    \n",
    "    for train_index, val_index in kFold.split(X):\n",
    "        #start timer, return avg time below\n",
    "        start = time()\n",
    "        \n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        Y_train, Y_val = Y[train_index], Y[val_index]\n",
    "        \n",
    "        # normalize data\n",
    "        X_train, X_val = normalizeData(X_train, X_val)\n",
    "        \n",
    "        # reduce dimensionality\n",
    "        if (ENABLE_PCN): X_train, X_val = pcaReduction(X_train, X_val, n_comp)\n",
    "        elif (ENABLE_LDA): X_train, X_val = ldaReduction(X_train, Y_train, X_val, n_comp)\n",
    "        \n",
    "        # build classifier for each set\n",
    "        clf = buildClf(X_train, Y_train, params)\n",
    "        \n",
    "        predicted = clf.predict(X_val)\n",
    "        val_accuracy = calcMetric(Y_val, predicted)\n",
    "        predicted = clf.predict(X_train)\n",
    "        train_accuracy = calcMetric(Y_train, predicted)\n",
    "        \n",
    "        # log loss calculation - for classification only\n",
    "        if(not ENABLE_REGRESSION_TARGET): log_loss_results.append(log_loss(Y_val, clf.predict_proba(X_val), labels=labels))\n",
    "        \n",
    "        val_results.append(val_accuracy)\n",
    "        train_results.append(train_accuracy)\n",
    "        time_log.append(time()-start)\n",
    "        \n",
    "    return np.mean(train_results, axis = 0), np.mean(val_results, axis = 0), np.mean(log_loss_results, axis = 0), np.mean(time_log, axis = 0)\n",
    "\n",
    "def looValidation(n_comp, params):\n",
    "    loo = LeaveOneOut()\n",
    "    \n",
    "    # run on validation data\n",
    "    val_results = []\n",
    "    train_results = []\n",
    "    Y_test_prob = []\n",
    "    log_loss_value = 0\n",
    "    time_log = []\n",
    "    \n",
    "    for train_index, val_index in loo.split(X):\n",
    "        #start timer, return avg time below\n",
    "        start = time()\n",
    "        \n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        Y_train, Y_val = Y[train_index], Y[val_index]\n",
    "        \n",
    "        # normalize data\n",
    "        X_train, X_val = normalizeData(X_train, X_val)\n",
    "        \n",
    "        # reduce dimensionality\n",
    "        if (ENABLE_PCN): X_train, X_val = pcaReduction(X_train, X_val, n_comp)\n",
    "        elif (ENABLE_LDA): X_train, X_val = ldaReduction(X_train, Y_train, X_val, n_comp)\n",
    "        \n",
    "        # build classifier for each set\n",
    "        clf = buildClf(X_train, Y_train, params)\n",
    "        \n",
    "        predicted = clf.predict(X_val)\n",
    "        val_accuracy = calcMetric(Y_val, predicted)\n",
    "        predicted = clf.predict(X_train)\n",
    "        train_accuracy = calcMetric(Y_train, predicted)\n",
    "        \n",
    "        # save probability for log loss calculation\n",
    "        if(not ENABLE_REGRESSION_TARGET): Y_test_prob.append(clf.predict_proba(X_val)[0])\n",
    "            \n",
    "        test_results.append(test_accuracy)\n",
    "        val_results.append(val_accuracy)\n",
    "        train_results.append(train_accuracy)\n",
    "        time_log.append(time()-start)\n",
    "    \n",
    "    # log loss calculation - for classification only\n",
    "    if(not ENABLE_REGRESSION_TARGET): log_loss_value = log_loss(Y, Y_test_prob, labels=labels)\n",
    "\n",
    "    return np.mean(train_results, axis = 0), np.mean(val_results, axis = 0), log_loss_value, np.mean(time_log, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evan/.virtualenvs/machine_intelligence/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param</th>\n",
       "      <th>Time</th>\n",
       "      <th>Acc</th>\n",
       "      <th>logloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[3, 4, 5, 5, 5, 7, 10, 8, 6, 9, 7, 9]</td>\n",
       "      <td>0.480954</td>\n",
       "      <td>0.769456</td>\n",
       "      <td>0.631085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5, 6, 3, 5, 8, 5, 1, 2, 6, 6, 6, 9]</td>\n",
       "      <td>0.147036</td>\n",
       "      <td>0.743145</td>\n",
       "      <td>0.571685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[3, 4, 5, 5, 5, 7, 10, 8, 6, 9, 7, 9]</td>\n",
       "      <td>0.427350</td>\n",
       "      <td>0.778226</td>\n",
       "      <td>0.531637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5, 6, 3, 5, 8, 5, 8, 2, 6, 6, 6, 9]</td>\n",
       "      <td>0.316573</td>\n",
       "      <td>0.702319</td>\n",
       "      <td>1.003604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[7, 6, 2, 5, 8, 5, 5, 3, 6, 6, 5, 9]</td>\n",
       "      <td>0.195113</td>\n",
       "      <td>0.667742</td>\n",
       "      <td>0.643921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[7, 6, 2, 6, 8, 1, 5, 3, 6, 6, 6, 9]</td>\n",
       "      <td>0.216267</td>\n",
       "      <td>0.690222</td>\n",
       "      <td>0.710638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[3, 4, 5, 2, 5, 7, 10, 8, 6, 9, 7, 9]</td>\n",
       "      <td>0.917427</td>\n",
       "      <td>0.781552</td>\n",
       "      <td>0.702376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[3, 4, 5, 5, 5, 7, 10, 8, 6, 9, 7, 9]</td>\n",
       "      <td>0.707696</td>\n",
       "      <td>0.721270</td>\n",
       "      <td>0.921128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[7, 6, 2, 5, 8, 5, 5, 3, 6, 6, 6, 9]</td>\n",
       "      <td>0.300629</td>\n",
       "      <td>0.816331</td>\n",
       "      <td>0.936563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[3, 4, 5, 5, 5, 7, 10, 8, 6, 9, 7, 9]</td>\n",
       "      <td>0.361222</td>\n",
       "      <td>0.794153</td>\n",
       "      <td>0.665345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[6, 9, 5, 5, 5, 7, 10, 4, 2, 3, 8, 9]</td>\n",
       "      <td>0.208407</td>\n",
       "      <td>0.769153</td>\n",
       "      <td>1.144498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[3, 5, 7, 5, 7, 7, 9, 8, 6, 9, 4, 10]</td>\n",
       "      <td>0.296894</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.960347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[3, 4, 5, 5, 8, 5, 10, 7, 6, 9, 7, 9]</td>\n",
       "      <td>0.365836</td>\n",
       "      <td>0.736593</td>\n",
       "      <td>1.362399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5, 6, 2, 5, 5, 7, 8, 3, 6, 6, 6, 9]</td>\n",
       "      <td>0.230931</td>\n",
       "      <td>0.762097</td>\n",
       "      <td>0.929215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5, 6, 3, 5, 8, 5, 1, 2, 6, 6, 6, 9]</td>\n",
       "      <td>0.073720</td>\n",
       "      <td>0.685887</td>\n",
       "      <td>0.724672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[7, 6, 2, 5, 8, 5, 5, 3, 6, 6, 6, 9]</td>\n",
       "      <td>0.380664</td>\n",
       "      <td>0.744355</td>\n",
       "      <td>1.429944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[8, 6, 2, 5, 3, 7, 10, 7, 6, 6, 6, 9]</td>\n",
       "      <td>0.234965</td>\n",
       "      <td>0.771673</td>\n",
       "      <td>0.897906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[7, 4, 5, 5, 10, 5, 8, 3, 6, 9, 7, 9]</td>\n",
       "      <td>0.307312</td>\n",
       "      <td>0.769153</td>\n",
       "      <td>1.111852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5, 6, 3, 5, 8, 5, 1, 2, 6, 6, 6, 9]</td>\n",
       "      <td>0.068820</td>\n",
       "      <td>0.660887</td>\n",
       "      <td>0.652009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[3, 4, 5, 5, 5, 7, 10, 8, 6, 9, 7, 1]</td>\n",
       "      <td>0.166077</td>\n",
       "      <td>0.670060</td>\n",
       "      <td>0.823925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5, 6, 2, 5, 8, 5, 1, 3, 6, 6, 6, 9]</td>\n",
       "      <td>0.016520</td>\n",
       "      <td>0.648387</td>\n",
       "      <td>0.650541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5, 6, 3, 5, 8, 1, 5, 2, 6, 6, 6, 9]</td>\n",
       "      <td>0.091511</td>\n",
       "      <td>0.683266</td>\n",
       "      <td>0.755129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5, 6, 3, 5, 8, 5, 1, 2, 6, 6, 6, 9]</td>\n",
       "      <td>0.057081</td>\n",
       "      <td>0.660887</td>\n",
       "      <td>0.644990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[7, 6, 2, 5, 8, 5, 5, 3, 6, 6, 6, 9]</td>\n",
       "      <td>0.184443</td>\n",
       "      <td>0.696673</td>\n",
       "      <td>1.033599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[8, 6, 2, 5, 5, 7, 10, 3, 6, 6, 6, 9]</td>\n",
       "      <td>0.253141</td>\n",
       "      <td>0.769153</td>\n",
       "      <td>0.885665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[3, 4, 5, 5, 10, 5, 8, 7, 6, 9, 7, 9]</td>\n",
       "      <td>0.272240</td>\n",
       "      <td>0.762298</td>\n",
       "      <td>0.812980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[7, 6, 2, 5, 8, 5, 6, 3, 6, 6, 5, 9]</td>\n",
       "      <td>0.241040</td>\n",
       "      <td>0.750504</td>\n",
       "      <td>1.213925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[8, 4, 5, 5, 5, 7, 10, 2, 9, 6, 7, 9]</td>\n",
       "      <td>0.181773</td>\n",
       "      <td>0.749698</td>\n",
       "      <td>0.680206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5, 6, 3, 5, 8, 5, 1, 2, 6, 6, 6, 9]</td>\n",
       "      <td>0.172226</td>\n",
       "      <td>0.709274</td>\n",
       "      <td>0.674542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[4, 10, 2, 5, 8, 5, 5, 3, 5, 9, 6, 9]</td>\n",
       "      <td>0.130596</td>\n",
       "      <td>0.685887</td>\n",
       "      <td>0.935115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   param      Time       Acc   logloss\n",
       "0  [3, 4, 5, 5, 5, 7, 10, 8, 6, 9, 7, 9]  0.480954  0.769456  0.631085\n",
       "0   [5, 6, 3, 5, 8, 5, 1, 2, 6, 6, 6, 9]  0.147036  0.743145  0.571685\n",
       "0  [3, 4, 5, 5, 5, 7, 10, 8, 6, 9, 7, 9]  0.427350  0.778226  0.531637\n",
       "0   [5, 6, 3, 5, 8, 5, 8, 2, 6, 6, 6, 9]  0.316573  0.702319  1.003604\n",
       "0   [7, 6, 2, 5, 8, 5, 5, 3, 6, 6, 5, 9]  0.195113  0.667742  0.643921\n",
       "0   [7, 6, 2, 6, 8, 1, 5, 3, 6, 6, 6, 9]  0.216267  0.690222  0.710638\n",
       "0  [3, 4, 5, 2, 5, 7, 10, 8, 6, 9, 7, 9]  0.917427  0.781552  0.702376\n",
       "0  [3, 4, 5, 5, 5, 7, 10, 8, 6, 9, 7, 9]  0.707696  0.721270  0.921128\n",
       "0   [7, 6, 2, 5, 8, 5, 5, 3, 6, 6, 6, 9]  0.300629  0.816331  0.936563\n",
       "0  [3, 4, 5, 5, 5, 7, 10, 8, 6, 9, 7, 9]  0.361222  0.794153  0.665345\n",
       "0  [6, 9, 5, 5, 5, 7, 10, 4, 2, 3, 8, 9]  0.208407  0.769153  1.144498\n",
       "0  [3, 5, 7, 5, 7, 7, 9, 8, 6, 9, 4, 10]  0.296894  0.762500  0.960347\n",
       "0  [3, 4, 5, 5, 8, 5, 10, 7, 6, 9, 7, 9]  0.365836  0.736593  1.362399\n",
       "0   [5, 6, 2, 5, 5, 7, 8, 3, 6, 6, 6, 9]  0.230931  0.762097  0.929215\n",
       "0   [5, 6, 3, 5, 8, 5, 1, 2, 6, 6, 6, 9]  0.073720  0.685887  0.724672\n",
       "0   [7, 6, 2, 5, 8, 5, 5, 3, 6, 6, 6, 9]  0.380664  0.744355  1.429944\n",
       "0  [8, 6, 2, 5, 3, 7, 10, 7, 6, 6, 6, 9]  0.234965  0.771673  0.897906\n",
       "0  [7, 4, 5, 5, 10, 5, 8, 3, 6, 9, 7, 9]  0.307312  0.769153  1.111852\n",
       "0   [5, 6, 3, 5, 8, 5, 1, 2, 6, 6, 6, 9]  0.068820  0.660887  0.652009\n",
       "0  [3, 4, 5, 5, 5, 7, 10, 8, 6, 9, 7, 1]  0.166077  0.670060  0.823925\n",
       "0   [5, 6, 2, 5, 8, 5, 1, 3, 6, 6, 6, 9]  0.016520  0.648387  0.650541\n",
       "0   [5, 6, 3, 5, 8, 1, 5, 2, 6, 6, 6, 9]  0.091511  0.683266  0.755129\n",
       "0   [5, 6, 3, 5, 8, 5, 1, 2, 6, 6, 6, 9]  0.057081  0.660887  0.644990\n",
       "0   [7, 6, 2, 5, 8, 5, 5, 3, 6, 6, 6, 9]  0.184443  0.696673  1.033599\n",
       "0  [8, 6, 2, 5, 5, 7, 10, 3, 6, 6, 6, 9]  0.253141  0.769153  0.885665\n",
       "0  [3, 4, 5, 5, 10, 5, 8, 7, 6, 9, 7, 9]  0.272240  0.762298  0.812980\n",
       "0   [7, 6, 2, 5, 8, 5, 6, 3, 6, 6, 5, 9]  0.241040  0.750504  1.213925\n",
       "0  [8, 4, 5, 5, 5, 7, 10, 2, 9, 6, 7, 9]  0.181773  0.749698  0.680206\n",
       "0   [5, 6, 3, 5, 8, 5, 1, 2, 6, 6, 6, 9]  0.172226  0.709274  0.674542\n",
       "0  [4, 10, 2, 5, 8, 5, 5, 3, 5, 9, 6, 9]  0.130596  0.685887  0.935115"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evalFinalPopulation(pop):\n",
    "    cols = ['param', 'Time', 'Acc', 'logloss']\n",
    "    all_results = pd.DataFrame(columns=cols)\n",
    "    for ind in best_pop:\n",
    "        train_res, val_res, log_loss_val, time_val = kFoldValidation(0, ind, 10)\n",
    "        all_results = all_results.append(pd.DataFrame([[ind, time_val, val_res, log_loss_val]], columns=cols))\n",
    "    return all_results\n",
    "\n",
    "best_pop = populations[-1]\n",
    "best_results = evalFinalPopulation(best_pop)\n",
    "best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8163306451612902"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(best_results.Acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain Accuracy, Log Loss & Error\n",
    "* Adjust array of penalty parameters\n",
    "* Graphs error and log loss - for classification only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# input reduced dimension - this can be ignored if none selected\n",
    "n_comp = 45\n",
    "\n",
    "# input parameter iterations - can tune other params in classifer selection method above\n",
    "if(not ENABLE_TEST):\n",
    "    # hyperparams = [None]\n",
    "    hyperparams = [(5, 0, 3)]\n",
    "else: hyperparams = [(5)] # input best parameter for test\n",
    "\n",
    "# results array format: [[train], [validation], [log loss]]\n",
    "results = [[],[],[]]\n",
    "\n",
    "# calculate train error, test error, log loss & time for specific param\n",
    "for penalty in hyperparams:\n",
    "    \n",
    "    if (ENABLE_KFOLD): train_res, val_res, log_loss_val, time_val = kFoldValidation(n_comp, penalty, 10)\n",
    "    elif (ENABLE_LOO): train_res, val_res, log_loss_val, time_val = looValidation(n_comp, penalty)\n",
    "    \n",
    "    # save error, RMSE, log loss for each penalty for graph\n",
    "    if (not ENABLE_REGRESSION_TARGET):\n",
    "        results[0].append(1-train_res)\n",
    "        results[1].append(1-val_res)\n",
    "        results[2].append(log_loss_val)\n",
    "    elif (ENABLE_REGRESSION_TARGET):\n",
    "        results[0].append(train_res)\n",
    "        results[1].append(val_res)\n",
    "\n",
    "    print (\"-----C={}-----\".format(penalty))\n",
    "    print (\"Time: {} seconds\".format(time_val))\n",
    "    print (\"-----Train-----\")\n",
    "    print (\"Accuracy/RMSE: {}\".format(train_res))\n",
    "    print (\"-----Validation-----\")\n",
    "    print (\"Accuracy/RMSE: {}\".format(val_res))\n",
    "    print (\"Log Loss: {}\\n\".format(log_loss_val))\n",
    "    \n",
    "# run best model on unseen test set\n",
    "if(ENABLE_TEST):\n",
    "    test_accuracy, test_log_loss, test_time = runTestSet(X, Y, X_tst, Y_tst, n_comp, hyperparams[0])\n",
    "\n",
    "    print (\"-----Test-----\")\n",
    "    print (\"Accuracy/RMSE: {}\".format(test_accuracy))\n",
    "    print (\"Log Loss: {}\".format(test_log_loss))\n",
    "    print (\"Time: {} seconds\".format(test_time))\n",
    "\n",
    "# create error and log loss graph for penalty iterations - classification only\n",
    "if(not ENABLE_REGRESSION_TARGET and len(hyperparams) > 1):\n",
    "    f, axarr = plt.subplots(2, sharex=False)\n",
    "    f.suptitle('Error and Log Loss', y = 0.92)\n",
    "    f.set_size_inches(10, 10)\n",
    "\n",
    "    # subplot 1: error plot\n",
    "    axarr[0].set_ylabel('Error')\n",
    "    axarr[0].plot(hyperparams, results[0], color='r', label='train')\n",
    "    axarr[0].plot(hyperparams, results[1], color='b', label='validation')\n",
    "    axarr[0].set_xticks(hyperparams)\n",
    "    axarr[0].legend()\n",
    "\n",
    "    # subplot 2: log loss plot\n",
    "    axarr[1].set_ylabel('Log Loss')\n",
    "    axarr[1].plot(hyperparams, results[2], color='g', label='log loss')\n",
    "    axarr[1].set_xticks(hyperparams)\n",
    "    axarr[1].set_xlabel('Parameter')\n",
    "    plt.show()\n",
    "    \n",
    "# create RMSE graph for penalty iterations - regression only\n",
    "if(ENABLE_REGRESSION_TARGET and len(hyperparams) > 1):\n",
    "    f, axarr = plt.subplots(sharex=False)\n",
    "    f.suptitle('RMSE', y = 0.92)\n",
    "    f.set_size_inches(10, 5)\n",
    "\n",
    "    # subplot 1: RMSE plot\n",
    "    axarr.set_ylabel('RMSE')\n",
    "    axarr.plot(hyperparams, results[0], color='r', label='train')\n",
    "    axarr.plot(hyperparams, results[1], color='b', label='validation')\n",
    "    axarr.set_xticks(hyperparams)\n",
    "    axarr.set_xlabel('Parameter')\n",
    "    axarr.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Test Method\n",
    "* Run best classifier on unseen test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get accuracy/RMSE, logloss and time \n",
    "def runTestSet(X_train, Y_train, X_test, Y_test, n_comp, params):\n",
    "    \n",
    "    start = time()\n",
    "    test_log_loss = 0\n",
    "    \n",
    "    # normalize data\n",
    "    X_train, X_test = normalizeData(X_train, X_test)\n",
    "\n",
    "    # reduce dimensionality\n",
    "    if (ENABLE_PCN): X_train, X_test = pcaReduction(X_train, X_test, n_comp)\n",
    "    elif (ENABLE_LDA): X_train, X_test = ldaReduction(X_train, Y_train, X_test, n_comp)\n",
    "\n",
    "    # build classifier, predict and get accuracy\n",
    "    clf = buildClf(X_train, Y_train, params)\n",
    "    predicted = clf.predict(X_test)\n",
    "    test_accuracy = calcMetric(Y_test, predicted)\n",
    "    \n",
    "    # log loss calculation - for classification only\n",
    "    if(not ENABLE_REGRESSION_TARGET): test_log_loss = log_loss(Y_test, clf.predict_proba(X_test))\n",
    "        \n",
    "    # execution time calculation\n",
    "    test_time = time()-start\n",
    "    \n",
    "    return test_accuracy, test_log_loss, test_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
